{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a prediction with Alphafold\n",
    "\n",
    "This notebook is created as a design document for making a prediction with alphafold in deepchem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies\n",
    "\n",
    "Extra dependencies for alphafold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%shell sudo apt install hmmer\n",
    "\n",
    "%shell conda install -y -q -c conda-forge -c bioconda \\\n",
    "    kalign2=2.04 \\\n",
    "    hhsuite=3.3.0 \\\n",
    "    openmm=7.5.1 \\\n",
    "    pdbfixer=1.7\n",
    "    \n",
    "%shell pip install -q \\\n",
    "    ml-collections==0.1.0 \\\n",
    "    PyYAML==5.4.1 \\\n",
    "    py3dmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'MAAHKGAEHHHKAAEHHEQAAKHHHAAAEHHEKGEHEQAAHHADTAYAHHKHAEEHAAQAAKHDAEHHAPKPH'\n",
    "num_residues = len(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search against genetic databases\n",
    "\n",
    "Use jackhmmr to identify homologous protein structures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat.sequence_featurizers.alphafold_featurizer import jackhmmer\n",
    "\n",
    "msas, deletion_matrices = jackhmmer(sequence, dbs = ['uniref90', 'smallbfd', 'mgnify'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat.sequence_featurizers import template_featurizer, data_pipeline, feature_pipeline\n",
    "\n",
    "template_feat = template_featurizer(mmcif_dir = '/data/mmcif', obsolete_pdbs_path = '/data/obs_pdbs', max_template_date = '2100-01-01', max_hits = 20) # optional\n",
    "\n",
    "data_pipe = data_pipeline(template_featurizer = template_feat) #templates are optional\n",
    "\n",
    "feature_dict = {}\n",
    "feature_dict.update(data_pipe.make_sequence_features(sequence, 'test', num_residues))\n",
    "feature_dict.update(data_pipe.make_msa_features(msas, deletion_matrices=deletion_matrices))\n",
    "\n",
    "feat_pipe = feature_pipeline(config='default')\n",
    "processed_feature_dict = feat_pipe.process_features(feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.layers import EvoformerLayer\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "ALPHAFOLD_PARAM_SOURCE_URL = 'https://storage.googleapis.com/alphafold/alphafold_params_2022-01-19.tar'\n",
    "\n",
    "class EvoFormerStack(nn.Module):\n",
    "  \"\"\"Stack of EvoFormer layers.\"\"\"\n",
    "\n",
    "  def __init__(self, num_layers, num_heads, d_model, d_ff, dropout_rate):\n",
    "    super().__init__()\n",
    "    self.layers = nn.ModuleList([\n",
    "        EvoformerLayer(num_heads, d_model, d_ff, dropout_rate)\n",
    "        for _ in range(num_layers)\n",
    "    ])\n",
    "\n",
    "  def forward(self, x, msa, mask):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, msa, mask)\n",
    "    return x\n",
    "\n",
    "class AlphaFold(nn.Module): # or nn.module and create lightning wrapper?\n",
    "  def __init__(self,config):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    config: dict\n",
    "      Model configuration\n",
    "    \"\"\"\n",
    "  def download_alphafold_params(self, url=ALPHAFOLD_PARAM_SOURCE_URL):\n",
    "    \"\"\"Downloads AlphaFold parameters from a URL.\"\"\"\n",
    "    \n",
    "  def iteration(self, feats, prevs, recycle=True):\n",
    "    \"\"\"Runs a single iteration of the model.\"\"\"\n",
    "    pass\n",
    "  def forward(self,batch):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "    batch: dict\n",
    "      Batch of data\n",
    "    \"\"\"\n",
    "    # extract data from batch\n",
    "    # pass through model\n",
    "    # recycle data\n",
    "    # return output\n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemfold = AlphaFold(config='default')\n",
    "chemfold.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "  chemfold.eval()\n",
    "  output = chemfold(processed_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.utils import protein # protein helper functions, with protein dataclass\n",
    "import py3Dmol\n",
    "\n",
    "unrelaxed_protein = protein.from_prediction(processed_feature_dict, output)\n",
    "visualize_pdb =  protein.visualize(unrelaxed_protein)\n",
    "\n",
    "view = py3Dmol.view(width=800, height=600)\n",
    "view.addModelsAsFrames(visualize_pdb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deepchem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de1ce6b2bfa2f3a8ef8fcc42be89d53aa54ed722391a62ff82bffbba164ea070"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
