{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "/home/tony/miniconda3/envs/deepchem/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.models.torch_models.infograph import InfoGraphModel\n",
    "from deepchem.feat import MolGraphConvFeaturizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/github/deepchem/deepchem/models/torch_models/infograph.py:163: UserWarning: Using a target size (torch.Size([800, 3])) that is different to the input size (torch.Size([800, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  sup_loss = F.mse_loss(self.model(inputs), labels)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m sep_encoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m InfoGraphModel(num_feat, num_edge, dim, use_unsup_loss, sep_encoder, tensorboard\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_dir\u001b[39m=\u001b[39msave_dir, batch_size\u001b[39m=\u001b[39m\u001b[39m800\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_dataset, nb_epoch\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/github/deepchem/deepchem/models/torch_models/torch_model.py:335\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    287\u001b[0m         dataset: Dataset,\n\u001b[1;32m    288\u001b[0m         nb_epoch: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[39m=\u001b[39m [],\n\u001b[1;32m    296\u001b[0m         all_losses: Optional[List[\u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m    297\u001b[0m     \u001b[39m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \n\u001b[1;32m    299\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m    The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_generator(\n\u001b[1;32m    336\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdefault_generator(dataset,\n\u001b[1;32m    337\u001b[0m                                epochs\u001b[39m=\u001b[39;49mnb_epoch,\n\u001b[1;32m    338\u001b[0m                                deterministic\u001b[39m=\u001b[39;49mdeterministic),\n\u001b[1;32m    339\u001b[0m         max_checkpoints_to_keep, checkpoint_interval, restore, variables,\n\u001b[1;32m    340\u001b[0m         loss, callbacks, all_losses)\n",
      "File \u001b[0;32m~/github/deepchem/deepchem/models/torch_models/modular.py:295\u001b[0m, in \u001b[0;36mModularTorchModel.fit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    292\u001b[0m     inputs \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    294\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 295\u001b[0m batch_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(inputs, labels, weights)\n\u001b[1;32m    296\u001b[0m batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    297\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/github/deepchem/deepchem/models/torch_models/infograph.py:164\u001b[0m, in \u001b[0;36mInfoGraphModel.loss_func\u001b[0;34m(self, inputs, labels, weights)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_unsup_loss:\n\u001b[1;32m    163\u001b[0m     sup_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs), labels)\n\u001b[0;32m--> 164\u001b[0m     unsup_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munsup_loss(inputs)\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseparate_encoder:\n\u001b[1;32m    166\u001b[0m         unsup_sup_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munsup_sup_loss(inputs, labels, weights)\n",
      "File \u001b[0;32m~/github/deepchem/deepchem/models/torch_models/infograph.py:184\u001b[0m, in \u001b[0;36mInfoGraphModel.unsup_loss\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m l_enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents[\u001b[39m'\u001b[39m\u001b[39mlocal_d\u001b[39m\u001b[39m'\u001b[39m](M)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal:\n\u001b[0;32m--> 184\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_global_loss_(l_enc, g_enc, inputs\u001b[39m.\u001b[39;49mgraph_index)\n\u001b[1;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/github/deepchem/deepchem/models/torch_models/infograph.py:240\u001b[0m, in \u001b[0;36mInfoGraphModel.local_global_loss_\u001b[0;34m(self, l_enc, g_enc, batch)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mfor\u001b[39;00m nodeidx, graphidx \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch):\n\u001b[1;32m    239\u001b[0m     pos_mask[nodeidx][graphidx] \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m\n\u001b[0;32m--> 240\u001b[0m     neg_mask[nodeidx][graphidx] \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m    242\u001b[0m res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(l_enc, g_enc\u001b[39m.\u001b[39mt())\n\u001b[1;32m    244\u001b[0m E_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_positive_expectation(res \u001b[39m*\u001b[39m pos_mask)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_dir = '/home/tony/github/data/zinc'\n",
    "\n",
    "featurizer = MolGraphConvFeaturizer(use_edges=True)\n",
    "tasks, datasets, transformers = dc.molnet.load_zinc15(featurizer=featurizer, splitter='random', save_dir=save_dir)\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "num_feat = 30 # max([train_dataset.X[i].num_node_features for i in range(len(train_dataset))])\n",
    "num_edge = 11 # max([train_dataset.X[i].num_edge_features for i in range(len(train_dataset))])\n",
    "dim = 64\n",
    "use_unsup_loss = True\n",
    "sep_encoder = False\n",
    "model = InfoGraphModel(num_feat, num_edge, dim, use_unsup_loss, sep_encoder, tensorboard=True, model_dir=save_dir, batch_size=800)\n",
    "model.fit(train_dataset, nb_epoch=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de1ce6b2bfa2f3a8ef8fcc42be89d53aa54ed722391a62ff82bffbba164ea070"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
